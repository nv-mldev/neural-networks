\contentsline {chapter}{\numberline {1}Introduction to Machine Learning}{5}{chapter.1}%
\contentsline {section}{\numberline {1.1}What is Learning?}{5}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Core Elements of Learning}{5}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Key Questions in Learning}{5}{subsection.1.1.2}%
\contentsline {section}{\numberline {1.2}What is Reasoning?}{5}{section.1.2}%
\contentsline {section}{\numberline {1.3}From Animal Learning to Machine Learning}{5}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Example: Bait Shyness in Rats}{5}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Parallel: Spam Email Filtering}{6}{subsection.1.3.2}%
\contentsline {section}{\numberline {1.4}Types of Reasoning: Comprehensive Overview}{6}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Inductive Reasoning}{6}{subsection.1.4.1}%
\contentsline {subsubsection}{Examples Across Different Contexts}{6}{section*.2}%
\contentsline {subsubsection}{Applications in AI/ML}{6}{section*.3}%
\contentsline {subsection}{\numberline {1.4.2}Deductive Reasoning}{7}{subsection.1.4.2}%
\contentsline {subsubsection}{Examples Across Different Contexts}{7}{section*.4}%
\contentsline {subsubsection}{Applications in AI}{7}{section*.5}%
\contentsline {subsubsection}{Limitations}{7}{section*.6}%
\contentsline {subsection}{\numberline {1.4.3}Abductive Reasoning (Inference to Best Explanation)}{7}{subsection.1.4.3}%
\contentsline {subsubsection}{Key Characteristics}{7}{section*.7}%
\contentsline {subsubsection}{Examples Across Different Contexts}{8}{section*.8}%
\contentsline {subsubsection}{Applications in AI/ML}{8}{section*.9}%
\contentsline {subsection}{\numberline {1.4.4}Additional Reasoning Types}{8}{subsection.1.4.4}%
\contentsline {subsubsection}{Analogical Reasoning (Pattern Transfer)}{8}{section*.10}%
\contentsline {subsubsection}{Bayesian Reasoning (Probabilistic Prediction)}{8}{section*.11}%
\contentsline {subsubsection}{Causal Reasoning (Understanding Cause-and-Effect)}{8}{section*.12}%
\contentsline {subsection}{\numberline {1.4.5}Reasoning Capabilities Across Intelligence Types}{9}{subsection.1.4.5}%
\contentsline {subsection}{\numberline {1.4.6}Limitations of Inductive Reasoning}{9}{subsection.1.4.6}%
\contentsline {subsubsection}{Pigeon Superstition Experiment (B.F. Skinner)}{9}{section*.13}%
\contentsline {subsubsection}{Garcia \& Koelling Experiment (1966)}{9}{section*.14}%
\contentsline {section}{\numberline {1.5}Inductive Bias in Machine Learning}{10}{section.1.5}%
\contentsline {subsection}{\numberline {1.5.1}What is Inductive Bias?}{10}{subsection.1.5.1}%
\contentsline {subsection}{\numberline {1.5.2}Types of Inductive Biases}{10}{subsection.1.5.2}%
\contentsline {subsubsection}{Preference for Simpler Models (Occam's Razor)}{10}{section*.15}%
\contentsline {subsubsection}{Smoothness Assumption}{10}{section*.16}%
\contentsline {subsubsection}{Similar Features Should Have Similar Effects}{10}{section*.17}%
\contentsline {subsubsection}{Prior Knowledge About the Task (Domain-Specific Bias)}{10}{section*.18}%
\contentsline {subsubsection}{Invariance Bias (Translation, Rotation, Scale Invariance)}{11}{section*.19}%
\contentsline {subsubsection}{Sparsity Assumption}{11}{section*.20}%
\contentsline {subsection}{\numberline {1.5.3}Inductive Bias in Specific Architectures}{11}{subsection.1.5.3}%
\contentsline {subsubsection}{Convolutional Neural Networks (CNNs)}{11}{section*.21}%
\contentsline {subsubsection}{Recurrent Neural Networks (RNNs \& LSTMs)}{11}{section*.22}%
\contentsline {subsubsection}{Transformers (BERT, GPT)}{12}{section*.23}%
\contentsline {section}{\numberline {1.6}Mathematical Foundations of Learning}{12}{section.1.6}%
\contentsline {subsection}{\numberline {1.6.1}Learning as Optimization}{12}{subsection.1.6.1}%
\contentsline {subsubsection}{Components of a Learning System}{12}{section*.24}%
\contentsline {subsubsection}{The Bias-Variance Tradeoff}{12}{section*.25}%
\contentsline {subsection}{\numberline {1.6.2}PAC Learning Framework}{12}{subsection.1.6.2}%
\contentsline {section}{\numberline {1.7}Symbolic AI vs Machine Learning}{13}{section.1.7}%
\contentsline {subsection}{\numberline {1.7.1}What is Symbolic AI?}{13}{subsection.1.7.1}%
\contentsline {subsection}{\numberline {1.7.2}Symbolic AI vs Machine Learning Comparison}{13}{subsection.1.7.2}%
\contentsline {chapter}{\numberline {2}Foundations of Computation}{15}{chapter.2}%
\contentsline {section}{\numberline {2.1}What is Computation?}{15}{section.2.1}%
\contentsline {section}{\numberline {2.2}Computational Models: Theoretical Foundations}{15}{section.2.2}%
\contentsline {section}{\numberline {2.3}Four Fundamental Computational Models}{15}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Turing Machine (Alan Turing, 1936)}{15}{subsection.2.3.1}%
\contentsline {subsubsection}{Core Characteristics}{15}{section*.26}%
\contentsline {subsubsection}{Strengths and Limitations}{15}{section*.27}%
\contentsline {subsection}{\numberline {2.3.2}Lambda Calculus (Alonzo Church, 1930s)}{16}{subsection.2.3.2}%
\contentsline {subsubsection}{Core Characteristics}{16}{section*.28}%
\contentsline {subsubsection}{Strengths and Limitations}{16}{section*.29}%
\contentsline {subsection}{\numberline {2.3.3}Cellular Automata (Stanislaw Ulam, John von Neumann, later Conway)}{16}{subsection.2.3.3}%
\contentsline {subsubsection}{Core Characteristics}{16}{section*.30}%
\contentsline {subsubsection}{Strengths and Limitations}{16}{section*.31}%
\contentsline {subsection}{\numberline {2.3.4}Biological Computation (Inspired by Nature)}{17}{subsection.2.3.4}%
\contentsline {subsubsection}{Core Characteristics}{17}{section*.32}%
\contentsline {subsubsection}{Examples of Biological Computation}{17}{section*.33}%
\contentsline {section}{\numberline {2.4}Biological Neural Networks: Nature's Computational Model}{17}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Characteristics of Biological Neural Networks}{17}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Neuron Structure}{18}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3}Neuron Structure and Components}{18}{subsection.2.4.3}%
\contentsline {subsubsection}{Typical Sizes}{18}{section*.34}%
\contentsline {subsection}{\numberline {2.4.4}Signal Transmission and Firing}{18}{subsection.2.4.4}%
\contentsline {subsection}{\numberline {2.4.5}Synapses: Chemistry and Types}{18}{subsection.2.4.5}%
\contentsline {subsection}{\numberline {2.4.6}Plasticity and Learning}{19}{subsection.2.4.6}%
\contentsline {section}{\numberline {2.5}Artificial Neural Networks}{19}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}Introduction: From Biology to Computation}{19}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}The Abstract Neuron: Building Block of Intelligence}{19}{subsection.2.5.2}%
\contentsline {subsubsection}{Mathematical Foundation: Linear Combination and Affine Transformation}{19}{section*.35}%
\contentsline {section}{\numberline {2.6}Activation Functions: Mathematical Properties}{19}{section.2.6}%
\contentsline {subsection}{\numberline {2.6.1}Common Activation Functions}{19}{subsection.2.6.1}%
\contentsline {subsubsection}{Step Function (Heaviside)}{19}{section*.36}%
\contentsline {subsubsection}{Sigmoid Function}{20}{section*.37}%
\contentsline {subsubsection}{Hyperbolic Tangent}{20}{section*.38}%
\contentsline {subsubsection}{Rectified Linear Unit (ReLU)}{20}{section*.39}%
\contentsline {subsection}{\numberline {2.6.2}Mathematical Requirements for Activation Functions}{20}{subsection.2.6.2}%
\contentsline {subsection}{\numberline {2.6.3}Neural Networks as Function Approximators}{20}{subsection.2.6.3}%
\contentsline {subsubsection}{Universal Approximation Theorem}{20}{section*.40}%
\contentsline {subsubsection}{Implications}{21}{section*.41}%
\contentsline {subsubsection}{Modern Extensions}{21}{section*.42}%
\contentsline {section}{\numberline {2.7}Artificial Neural Networks: From Theory to Implementation}{21}{section.2.7}%
\contentsline {subsection}{\numberline {2.7.1}Fundamental Architecture: Primitive Functions and Composition Rules}{21}{subsection.2.7.1}%
\contentsline {subsubsection}{Primitive Functions in Neural Networks}{21}{section*.43}%
\contentsline {subsubsection}{Composition Rules in Neural Networks}{21}{section*.44}%
\contentsline {subsection}{\numberline {2.7.2}Neural Networks as Function Approximators}{22}{subsection.2.7.2}%
\contentsline {subsubsection}{Networks of Primitive Functions}{22}{section*.45}%
\contentsline {subsubsection}{The Network Function}{22}{section*.46}%
\contentsline {subsubsection}{Three Critical Elements}{22}{section*.47}%
\contentsline {subsection}{\numberline {2.7.3}Function Approximation: The Classical Problem}{22}{subsection.2.7.3}%
\contentsline {subsubsection}{Historical Context}{22}{section*.48}%
\contentsline {subsubsection}{Neural Networks as Universal Approximators}{22}{section*.49}%
\contentsline {subsubsection}{Advantages of Neural Network Approximation}{23}{section*.50}%
\contentsline {subsection}{\numberline {2.7.4}Learning from Data: The Key Difference}{23}{subsection.2.7.4}%
\contentsline {subsubsection}{Classical Series vs. Neural Networks: A Fundamental Distinction}{23}{section*.51}%
\contentsline {subsubsection}{Mathematical Formulation of the Learning Problem}{23}{section*.52}%
\contentsline {subsubsection}{Generalization Gap}{24}{section*.53}%
\contentsline {section}{\numberline {2.8}Computational Complexity in Neural Networks}{24}{section.2.8}%
\contentsline {subsection}{\numberline {2.8.1}Forward Pass Complexity}{24}{subsection.2.8.1}%
\contentsline {subsection}{\numberline {2.8.2}Backward Pass Complexity (Backpropagation)}{24}{subsection.2.8.2}%
\contentsline {subsection}{\numberline {2.8.3}Scalability Considerations}{24}{subsection.2.8.3}%
\contentsline {chapter}{\numberline {3}The Perceptron}{25}{chapter.3}%
\contentsline {section}{\numberline {3.1}Historical Development of Neural Networks}{25}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Timeline of Neural Network Evolution}{25}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Threshold Logic: The Foundation}{25}{subsection.3.1.2}%
\contentsline {section}{\numberline {3.2}McCulloch-Pitts Neuron: The First Artificial Neuron}{25}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Mathematical Model}{26}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Logic Gate Implementation}{26}{subsection.3.2.2}%
\contentsline {subsubsection}{AND Gate}{26}{section*.54}%
\contentsline {subsubsection}{OR Gate}{26}{section*.55}%
\contentsline {subsection}{\numberline {3.2.3}Limitations of McCulloch-Pitts Neurons}{26}{subsection.3.2.3}%
\contentsline {section}{\numberline {3.3}The Perceptron: A Detailed Introduction}{26}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Definition: The Anatomy of a Perceptron}{27}{subsection.3.3.1}%
\contentsline {section}{\numberline {3.4}The Perceptron Learning Rule}{27}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Mathematical Derivation}{27}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Convergence Theorem (Rosenblatt, 1962)}{27}{subsection.3.4.2}%
\contentsline {subsubsection}{Proof Sketch}{27}{section*.56}%
\contentsline {subsubsection}{Convergence Bound}{27}{section*.57}%
\contentsline {subsection}{\numberline {3.4.3}Learning Rate Analysis}{28}{subsection.3.4.3}%
\contentsline {subsubsection}{Effect of Learning Rate \(\eta \)}{28}{section*.58}%
\contentsline {subsubsection}{Adaptive Learning Rates}{28}{section*.59}%
\contentsline {section}{\numberline {3.5}Vectorisation of Perceptron Learning Rule: A Matrix-Based Example}{28}{section.3.5}%
\contentsline {subsection}{\numberline {3.5.1}Mathematical Foundation of Vectorization}{28}{subsection.3.5.1}%
\contentsline {subsection}{\numberline {3.5.2}Define the Matrices}{28}{subsection.3.5.2}%
\contentsline {subsubsection}{Input Matrix \(X\) (Augmented Design Matrix)}{28}{section*.60}%
\contentsline {subsubsection}{Target Vector \(T\)}{29}{section*.61}%
\contentsline {subsubsection}{Weight Vector \(W\)}{29}{section*.62}%
\contentsline {subsection}{\numberline {3.5.3}Epoch 1: Mathematical Flow}{29}{subsection.3.5.3}%
\contentsline {subsubsection}{Step 1: Compute Net Input Z}{29}{section*.63}%
\contentsline {subsubsection}{Step 2: Apply Activation Function to get Output Y}{29}{section*.64}%
\contentsline {subsubsection}{Step 3: Calculate the Error Vector E}{29}{section*.65}%
\contentsline {subsubsection}{Step 4: Calculate the Total Weight Update \(\Delta W\)}{29}{section*.66}%
\contentsline {subsubsection}{Step 5: Update the Weight Vector W}{30}{section*.67}%
\contentsline {subsection}{\numberline {3.5.4}Mathematical Insight: Gradient Descent Connection}{30}{subsection.3.5.4}%
\contentsline {subsubsection}{Loss Function}{30}{section*.68}%
\contentsline {subsubsection}{Gradient of the Loss}{30}{section*.69}%
\contentsline {subsubsection}{Batch Update Rule}{30}{section*.70}%
\contentsline {subsection}{\numberline {3.5.5}Computational Complexity Analysis}{30}{subsection.3.5.5}%
\contentsline {subsubsection}{Vectorized vs. Sequential Processing}{30}{section*.71}%
\contentsline {section}{\numberline {3.6}Geometric Interpretation of the Perceptron}{31}{section.3.6}%
\contentsline {subsection}{\numberline {3.6.1}Mathematical Foundation of Decision Boundaries}{31}{subsection.3.6.1}%
\contentsline {subsubsection}{Hyperplane Properties}{31}{section*.72}%
\contentsline {subsubsection}{Margin and Support}{31}{section*.73}%
\contentsline {subsection}{\numberline {3.6.2}Example: The NOT Function}{31}{subsection.3.6.2}%
\contentsline {subsubsection}{The Input Space}{31}{section*.74}%
\contentsline {subsubsection}{The Weight Space}{31}{section*.75}%
\contentsline {subsection}{\numberline {3.6.3}Example: The AND Function}{31}{subsection.3.6.3}%
\contentsline {subsubsection}{The Input Space}{31}{section*.76}%
\contentsline {subsubsection}{The Weight Space}{31}{section*.77}%
\contentsline {section}{\numberline {3.7}Limitations of the Perceptron}{32}{section.3.7}%
\contentsline {subsection}{\numberline {3.7.1}Linear Separability Constraint}{32}{subsection.3.7.1}%
\contentsline {subsubsection}{Definition: Linear Separability}{32}{section*.78}%
\contentsline {subsubsection}{The XOR Problem (Minsky \& Papert, 1969)}{33}{section*.79}%
\contentsline {subsection}{\numberline {3.7.2}Solutions to Linear Separability Limitation}{34}{subsection.3.7.2}%
\contentsline {subsubsection}{Multi-Layer Perceptrons (MLPs)}{34}{section*.80}%
\contentsline {subsubsection}{Feature Engineering}{34}{section*.81}%
\contentsline {subsubsection}{Ensemble Methods}{34}{section*.82}%
\contentsline {section}{\numberline {3.8}Historical Impact and Legacy}{34}{section.3.8}%
\contentsline {subsection}{\numberline {3.8.1}The Perceptron Controversy}{34}{subsection.3.8.1}%
\contentsline {subsection}{\numberline {3.8.2}Modern Relevance}{35}{subsection.3.8.2}%
