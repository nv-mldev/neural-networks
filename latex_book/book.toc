\contentsline {chapter}{\numberline {1}Introduction to Machine Learning}{5}{chapter.1}%
\contentsline {section}{\numberline {1.1}What is Learning?}{5}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Core Elements of Learning}{5}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Key Questions in Learning}{5}{subsection.1.1.2}%
\contentsline {section}{\numberline {1.2}What is Reasoning?}{5}{section.1.2}%
\contentsline {section}{\numberline {1.3}Types of Reasoning: Comprehensive Overview}{5}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Inductive Reasoning}{6}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Deductive Reasoning}{6}{subsection.1.3.2}%
\contentsline {subsection}{\numberline {1.3.3}Abductive Reasoning (Inference to Best Explanation)}{6}{subsection.1.3.3}%
\contentsline {section}{\numberline {1.4}Animal Learning}{6}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Example: Bait Shyness in Rats}{6}{subsection.1.4.1}%
\contentsline {section}{\numberline {1.5}Human Learning: The Cognitive Approach}{6}{section.1.5}%
\contentsline {subsection}{\numberline {1.5.1}Cognitive Stages of Development}{6}{subsection.1.5.1}%
\contentsline {subsection}{\numberline {1.5.2}The Role of Memory}{7}{subsection.1.5.2}%
\contentsline {subsection}{\numberline {1.5.3}Learning Styles and Strategies}{7}{subsection.1.5.3}%
\contentsline {section}{\numberline {1.6}What is Machine Learning?}{7}{section.1.6}%
\contentsline {subsection}{\numberline {1.6.1}Parallel: Spam Email Filtering}{7}{subsection.1.6.1}%
\contentsline {section}{\numberline {1.7}Types of Machine Learning}{8}{section.1.7}%
\contentsline {subsection}{\numberline {1.7.1}Supervised Learning}{8}{subsection.1.7.1}%
\contentsline {subsection}{\numberline {1.7.2}Unsupervised Learning}{8}{subsection.1.7.2}%
\contentsline {subsection}{\numberline {1.7.3}Reinforcement Learning}{8}{subsection.1.7.3}%
\contentsline {section}{\numberline {1.8}The Data and Observation Model}{8}{section.1.8}%
\contentsline {subsection}{\numberline {1.8.1}The Design Matrix}{8}{subsection.1.8.1}%
\contentsline {section}{\numberline {1.9}Probabilistic Model of Learning}{8}{section.1.9}%
\contentsline {section}{\numberline {1.10}Inductive Bias in Machine Learning}{9}{section.1.10}%
\contentsline {subsection}{\numberline {1.10.1}What is Inductive Bias?}{9}{subsection.1.10.1}%
\contentsline {subsection}{\numberline {1.10.2}Types of Inductive Biases}{9}{subsection.1.10.2}%
\contentsline {subsubsection}{Preference for Simpler Models (Occam's Razor)}{9}{section*.2}%
\contentsline {subsubsection}{Smoothness Assumption}{9}{section*.3}%
\contentsline {subsection}{\numberline {1.10.3}Inductive Bias in Specific Architectures}{9}{subsection.1.10.3}%
\contentsline {subsubsection}{Convolutional Neural Networks (CNNs)}{9}{section*.4}%
\contentsline {subsubsection}{Recurrent Neural Networks (RNNs \& LSTMs)}{9}{section*.5}%
\contentsline {subsubsection}{Transformers (BERT, GPT)}{10}{section*.6}%
\contentsline {section}{\numberline {1.11}Mathematical Foundations of Learning}{10}{section.1.11}%
\contentsline {subsection}{\numberline {1.11.1}Learning as Optimization}{10}{subsection.1.11.1}%
\contentsline {subsubsection}{Components of a Learning System}{10}{section*.7}%
\contentsline {subsubsection}{The Bias-Variance Tradeoff}{10}{section*.8}%
\contentsline {subsection}{\numberline {1.11.2}PAC Learning Framework}{10}{subsection.1.11.2}%
\contentsline {section}{\numberline {1.12}Symbolic AI vs Machine Learning}{10}{section.1.12}%
\contentsline {subsection}{\numberline {1.12.1}What is Symbolic AI?}{10}{subsection.1.12.1}%
\contentsline {subsection}{\numberline {1.12.2}Symbolic AI vs Machine Learning Comparison}{10}{subsection.1.12.2}%
\contentsline {chapter}{\numberline {2}Foundations of Computation}{13}{chapter.2}%
\contentsline {section}{\numberline {2.1}What is Computation?}{13}{section.2.1}%
\contentsline {section}{\numberline {2.2}Computational Models: Theoretical Foundations}{13}{section.2.2}%
\contentsline {section}{\numberline {2.3}Four Fundamental Computational Models}{13}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Turing Machine (Alan Turing, 1936)}{13}{subsection.2.3.1}%
\contentsline {subsubsection}{Core Characteristics}{13}{section*.9}%
\contentsline {subsubsection}{Strengths and Limitations}{13}{section*.10}%
\contentsline {subsection}{\numberline {2.3.2}Lambda Calculus (Alonzo Church, 1930s)}{14}{subsection.2.3.2}%
\contentsline {subsubsection}{Core Characteristics}{14}{section*.11}%
\contentsline {subsubsection}{Strengths and Limitations}{14}{section*.12}%
\contentsline {subsection}{\numberline {2.3.3}Cellular Automata (Stanislaw Ulam, John von Neumann, later Conway)}{14}{subsection.2.3.3}%
\contentsline {subsubsection}{Core Characteristics}{14}{section*.13}%
\contentsline {subsubsection}{Strengths and Limitations}{14}{section*.14}%
\contentsline {subsection}{\numberline {2.3.4}Biological Computation (Inspired by Nature)}{15}{subsection.2.3.4}%
\contentsline {subsubsection}{Core Characteristics}{15}{section*.15}%
\contentsline {subsubsection}{Examples of Biological Computation}{15}{section*.16}%
\contentsline {section}{\numberline {2.4}Biological Neural Networks: Nature's Computational Model}{15}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Characteristics of Biological Neural Networks}{15}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Neuron Structure}{16}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3}Neuron Structure and Components}{16}{subsection.2.4.3}%
\contentsline {subsubsection}{Typical Sizes}{16}{section*.17}%
\contentsline {subsection}{\numberline {2.4.4}Signal Transmission and Firing}{16}{subsection.2.4.4}%
\contentsline {subsection}{\numberline {2.4.5}Synapses: Chemistry and Types}{16}{subsection.2.4.5}%
\contentsline {subsection}{\numberline {2.4.6}Plasticity and Learning}{17}{subsection.2.4.6}%
\contentsline {section}{\numberline {2.5}Artificial Neural Networks}{17}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}Introduction: From Biology to Computation}{17}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}The Abstract Neuron: Building Block of Intelligence}{17}{subsection.2.5.2}%
\contentsline {subsubsection}{Mathematical Foundation: Linear Combination and Affine Transformation}{17}{section*.18}%
\contentsline {section}{\numberline {2.6}Activation Functions: Mathematical Properties}{17}{section.2.6}%
\contentsline {subsection}{\numberline {2.6.1}Common Activation Functions}{17}{subsection.2.6.1}%
\contentsline {subsubsection}{Step Function (Heaviside)}{17}{section*.19}%
\contentsline {subsubsection}{Sigmoid Function}{18}{section*.20}%
\contentsline {subsubsection}{Hyperbolic Tangent}{18}{section*.21}%
\contentsline {subsubsection}{Rectified Linear Unit (ReLU)}{18}{section*.22}%
\contentsline {subsection}{\numberline {2.6.2}Mathematical Requirements for Activation Functions}{18}{subsection.2.6.2}%
\contentsline {subsection}{\numberline {2.6.3}Neural Networks as Function Approximators}{18}{subsection.2.6.3}%
\contentsline {subsubsection}{Universal Approximation Theorem}{18}{section*.23}%
\contentsline {subsubsection}{Implications}{19}{section*.24}%
\contentsline {subsubsection}{Modern Extensions}{19}{section*.25}%
\contentsline {section}{\numberline {2.7}Artificial Neural Networks: From Theory to Implementation}{19}{section.2.7}%
\contentsline {subsection}{\numberline {2.7.1}Fundamental Architecture: Primitive Functions and Composition Rules}{19}{subsection.2.7.1}%
\contentsline {subsubsection}{Primitive Functions in Neural Networks}{19}{section*.26}%
\contentsline {subsubsection}{Composition Rules in Neural Networks}{19}{section*.27}%
\contentsline {subsection}{\numberline {2.7.2}Neural Networks as Function Approximators}{20}{subsection.2.7.2}%
\contentsline {subsubsection}{Networks of Primitive Functions}{20}{section*.28}%
\contentsline {subsubsection}{The Network Function}{20}{section*.29}%
\contentsline {subsubsection}{Three Critical Elements}{20}{section*.30}%
\contentsline {subsection}{\numberline {2.7.3}Function Approximation: The Classical Problem}{20}{subsection.2.7.3}%
\contentsline {subsubsection}{Historical Context}{20}{section*.31}%
\contentsline {subsubsection}{Neural Networks as Universal Approximators}{20}{section*.32}%
\contentsline {subsubsection}{Advantages of Neural Network Approximation}{21}{section*.33}%
\contentsline {subsection}{\numberline {2.7.4}Learning from Data: The Key Difference}{21}{subsection.2.7.4}%
\contentsline {subsubsection}{Classical Series vs. Neural Networks: A Fundamental Distinction}{21}{section*.34}%
\contentsline {subsubsection}{Mathematical Formulation of the Learning Problem}{21}{section*.35}%
\contentsline {subsubsection}{Generalization Gap}{22}{section*.36}%
\contentsline {section}{\numberline {2.8}Computational Complexity in Neural Networks}{22}{section.2.8}%
\contentsline {subsection}{\numberline {2.8.1}Forward Pass Complexity}{22}{subsection.2.8.1}%
\contentsline {subsection}{\numberline {2.8.2}Backward Pass Complexity (Backpropagation)}{22}{subsection.2.8.2}%
\contentsline {subsection}{\numberline {2.8.3}Scalability Considerations}{22}{subsection.2.8.3}%
\contentsline {chapter}{\numberline {3}The Perceptron}{23}{chapter.3}%
\contentsline {section}{\numberline {3.1}Historical Development of Neural Networks}{23}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Timeline of Neural Network Evolution}{23}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Threshold Logic: The Foundation}{23}{subsection.3.1.2}%
\contentsline {section}{\numberline {3.2}McCulloch-Pitts Neuron: The First Artificial Neuron}{23}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Mathematical Model}{24}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Logic Gate Implementation}{24}{subsection.3.2.2}%
\contentsline {subsubsection}{AND Gate}{24}{section*.37}%
\contentsline {subsubsection}{OR Gate}{24}{section*.38}%
\contentsline {subsection}{\numberline {3.2.3}Limitations of McCulloch-Pitts Neurons}{24}{subsection.3.2.3}%
\contentsline {section}{\numberline {3.3}The Perceptron: A Detailed Introduction}{24}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Definition: The Anatomy of a Perceptron}{25}{subsection.3.3.1}%
\contentsline {section}{\numberline {3.4}The Perceptron Learning Rule}{25}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Mathematical Derivation}{25}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Convergence Theorem (Rosenblatt, 1962)}{25}{subsection.3.4.2}%
\contentsline {subsubsection}{Proof Sketch}{25}{section*.39}%
\contentsline {subsubsection}{Convergence Bound}{25}{section*.40}%
\contentsline {subsection}{\numberline {3.4.3}Learning Rate Analysis}{26}{subsection.3.4.3}%
\contentsline {subsubsection}{Effect of Learning Rate \(\eta \)}{26}{section*.41}%
\contentsline {subsubsection}{Adaptive Learning Rates}{26}{section*.42}%
\contentsline {section}{\numberline {3.5}Vectorisation of Perceptron Learning Rule: A Matrix-Based Example}{26}{section.3.5}%
\contentsline {subsection}{\numberline {3.5.1}Mathematical Foundation of Vectorization}{26}{subsection.3.5.1}%
\contentsline {subsection}{\numberline {3.5.2}Define the Matrices}{26}{subsection.3.5.2}%
\contentsline {subsubsection}{Input Matrix \(X\) (Augmented Design Matrix)}{26}{section*.43}%
\contentsline {subsubsection}{Target Vector \(T\)}{27}{section*.44}%
\contentsline {subsubsection}{Weight Vector \(W\)}{27}{section*.45}%
\contentsline {subsection}{\numberline {3.5.3}Epoch 1: Mathematical Flow}{27}{subsection.3.5.3}%
\contentsline {subsubsection}{Step 1: Compute Net Input Z}{27}{section*.46}%
\contentsline {subsubsection}{Step 2: Apply Activation Function to get Output Y}{27}{section*.47}%
\contentsline {subsubsection}{Step 3: Calculate the Error Vector E}{27}{section*.48}%
\contentsline {subsubsection}{Step 4: Calculate the Total Weight Update \(\Delta W\)}{27}{section*.49}%
\contentsline {subsubsection}{Step 5: Update the Weight Vector W}{28}{section*.50}%
\contentsline {subsection}{\numberline {3.5.4}Mathematical Insight: Gradient Descent Connection}{28}{subsection.3.5.4}%
\contentsline {subsubsection}{Loss Function}{28}{section*.51}%
\contentsline {subsubsection}{Gradient of the Loss}{28}{section*.52}%
\contentsline {subsubsection}{Batch Update Rule}{28}{section*.53}%
\contentsline {subsection}{\numberline {3.5.5}Computational Complexity Analysis}{28}{subsection.3.5.5}%
\contentsline {subsubsection}{Vectorized vs. Sequential Processing}{28}{section*.54}%
\contentsline {section}{\numberline {3.6}Geometric Interpretation of the Perceptron}{29}{section.3.6}%
\contentsline {subsection}{\numberline {3.6.1}Mathematical Foundation of Decision Boundaries}{29}{subsection.3.6.1}%
\contentsline {subsubsection}{Hyperplane Properties}{29}{section*.55}%
\contentsline {subsubsection}{Margin and Support}{29}{section*.56}%
\contentsline {subsection}{\numberline {3.6.2}Example: The NOT Function}{29}{subsection.3.6.2}%
\contentsline {subsubsection}{The Input Space}{29}{section*.57}%
\contentsline {subsubsection}{The Weight Space}{29}{section*.58}%
\contentsline {subsection}{\numberline {3.6.3}Example: The AND Function}{29}{subsection.3.6.3}%
\contentsline {subsubsection}{The Input Space}{29}{section*.59}%
\contentsline {subsubsection}{The Weight Space}{29}{section*.60}%
\contentsline {section}{\numberline {3.7}Limitations of the Perceptron}{30}{section.3.7}%
\contentsline {subsection}{\numberline {3.7.1}Linear Separability Constraint}{30}{subsection.3.7.1}%
\contentsline {subsubsection}{Definition: Linear Separability}{30}{section*.61}%
\contentsline {subsubsection}{The XOR Problem (Minsky \& Papert, 1969)}{31}{section*.62}%
\contentsline {subsection}{\numberline {3.7.2}Solutions to Linear Separability Limitation}{32}{subsection.3.7.2}%
\contentsline {subsubsection}{Multi-Layer Perceptrons (MLPs)}{32}{section*.63}%
\contentsline {subsubsection}{Feature Engineering}{32}{section*.64}%
\contentsline {subsubsection}{Ensemble Methods}{32}{section*.65}%
\contentsline {section}{\numberline {3.8}Historical Impact and Legacy}{32}{section.3.8}%
\contentsline {subsection}{\numberline {3.8.1}The Perceptron Controversy}{32}{subsection.3.8.1}%
\contentsline {subsection}{\numberline {3.8.2}Modern Relevance}{33}{subsection.3.8.2}%
\contentsline {section}{\numberline {3.9}Exercises}{33}{section.3.9}%
