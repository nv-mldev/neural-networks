\section{What is Learning?}
Learning is the process of acquiring new knowledge, skills, or behaviors through experience. This process transforms inputs---such as data, experiences, or information---into useful capabilities like expertise, new skills, or predictive models.

\subsection{Core Elements of Learning}
Every learning process involves these fundamental components:
\begin{enumerate}
    \item \textbf{Input}: Data, experiences, or information that enters the learning system
    \item \textbf{Processing}: The manipulation or transformation of that data according to learning rules
    \item \textbf{Output}: The result---new knowledge, skills, or predictive capabilities
    \item \textbf{Feedback}: Information about the effectiveness of learning outcomes
    \item \textbf{Memory}: The ability to retain and access learned information
\end{enumerate}

\subsection{Key Questions in Learning}
\begin{itemize}
    \item What are the essential inputs for the learning process?
    \item How do we measure the effectiveness and success of learning?
    \item What are the underlying mechanisms and processes by which learning occurs?
    \item How can we generalize from specific experiences to handle new situations?
\end{itemize}

\section{What is Reasoning?}
Reasoning is the ability to draw logical conclusions from known facts or learned knowledge. Unlike learning, reasoning relies on logical inference rather than large amounts of data.

\section{From Animal Learning to Machine Learning}
\subsection{Example: Bait Shyness in Rats}
Rats demonstrate a fundamental learning principle through their feeding behavior:
\begin{itemize}
    \item They sample novel food cautiously
    \item If the food causes illness, they avoid it in the future
    \item Past experience directly informs future decisions
\end{itemize}
This natural learning process parallels challenges in machine learning.

\subsection{Parallel: Spam Email Filtering}
Consider how this biological learning principle applies to spam detection:
\begin{itemize}
    \item \textbf{Naive approach}: Memorize all past spam emails
    \item \textbf{Problem}: Cannot classify previously unseen emails
    \item \textbf{Solution}: Extract generalizable patterns (like words, phrases, or sender patterns)
    \item \textbf{Key insight}: Both rats and spam filters must generalize from specific experiences to handle new, similar situations
\end{itemize}
This ability to generalize leads us to examine the different types of reasoning that enable learning and decision-making.

\section{Types of Reasoning: Comprehensive Overview}
Understanding different types of reasoning is crucial for designing effective learning systems. Each type has distinct characteristics and applications in both biological and artificial intelligence systems.

\subsection{Inductive Reasoning}
\textbf{Definition:} Inductive reasoning extracts patterns from observed data to make predictions about future or unseen cases. This approach moves from specific observations to general conclusions, yielding probable rather than certain results.

\textbf{Key Characteristics:}
\begin{itemize}
    \item Most prevalent form of reasoning in the animal kingdom and primary mode in machine learning
    \item Forms the basis of most learned behaviors in animals
    \item Used extensively in deep learning and LLMs
    \item Enables generalization from limited examples to broader patterns
\end{itemize}

\subsubsection{Examples Across Different Contexts}
\textbf{Human Example:} Every cat I have ever seen has four legs. Therefore, all cats have four legs.

\textbf{Animal Example:}
\begin{itemize}
    \item A dog learns that when its owner picks up the leash, it will probably go for a walk (experienced hundreds of times)
    \item A squirrel learns that acorns are edible after eating many without getting sick
\end{itemize}

\textbf{Machine Learning Example:} A spam classifier learns from previously labeled emails and generalizes patterns to detect new spam messages.

\textbf{Large Language Model Example:} When asked to complete "The sky is blue because...", the model has "observed" this pattern countless times in training data and induces probable completions based on statistical patterns.

\subsubsection{Applications in AI/ML}
\begin{itemize}
    \item Deep learning model training
    \item Pattern recognition systems
    \item Predictive analytics
    \item Natural language processing
\end{itemize}

\subsection{Deductive Reasoning}
\textbf{Definition:} Deductive reasoning moves from general rules and premises to reach specific, guaranteed conclusions. It starts with a general rule and a specific case to reach a logical conclusion.

\textbf{Key Characteristics:}
\begin{itemize}
    \item Provides certainty when premises are true
    \item Animals generally lack this capability for abstract reasoning
    \item LLMs can only mimic this through pattern matching
    \item Forms the basis of formal logic and mathematical proof
\end{itemize}

\subsubsection{Examples Across Different Contexts}
\textbf{Human Example:} All men are mortal. Socrates is a man. Therefore, Socrates is mortal.

\textbf{Mathematical Example:} If all squares have four sides and a shape is a square, it must have four sides.

\textbf{Animal Limitation:} Animals cannot perform abstract syllogistic reasoning, such as deducing that "because all felines are carnivores and a tiger is a feline, then a tiger is a carnivore."

\textbf{LLM Mimicry:} When given "All mammals have hair. A dolphin is a mammal. Therefore...", the model completes with "a dolphin has hair"---not through true logical syllogism, but by recognizing learned textual patterns from training data.

\subsubsection{Applications in AI}
\begin{itemize}
    \item Expert systems (traditional AI)
    \item Symbolic reasoning systems
    \item Theorem proving
    \item Rule-based systems
\end{itemize}

\subsubsection{Limitations}
\begin{itemize}
    \item LLMs lack strict logical reasoning capabilities
    \item Most modern AI systems don't use true deductive reasoning
    \item Requires explicit knowledge representation
\end{itemize}

\subsection{Abductive Reasoning (Inference to Best Explanation)}
\textbf{Definition:} Abductive reasoning starts with an observation and seeks to find the simplest and most likely explanation. It's the process of finding a hypothesis that, if true, would best explain the observation.

\subsubsection{Key Characteristics}
\begin{itemize}
    \item Often described as "inference to the best explanation"
    \item Guesses the most probable explanation given incomplete data
    \item Can be demonstrated in simple forms by animals
    \item Simulated effectively by modern LLMs
\end{itemize}

\subsubsection{Examples Across Different Contexts}
\textbf{Human Example:} The grass is wet. A plausible explanation is that it rained (most likely, though sprinklers are possible).

\textbf{Medical Example:} A doctor observes symptoms like fever and cough and infers the patient likely has the flu.

\textbf{Animal Example:}
\begin{itemize}
    \item A squirrel hears rustling and sees movement, "abduces" it's a predator and climbs a tree
    \item A raven sees a human place a rock over food, infers the food is under the rock when human leaves
\end{itemize}

\textbf{LLM Example:} When asked "Why is the road wet?", generates explanations like "It rained," "Water main broke," or "Street cleaner passed" by ranking probable explanations from training data.

\subsubsection{Applications in AI/ML}
\begin{itemize}
    \item Medical diagnosis systems
    \item Troubleshooting AI
    \item Natural language understanding
    \item Creative writing and content generation
\end{itemize}

\subsection{Additional Reasoning Types}

\subsubsection{Analogical Reasoning (Pattern Transfer)}
\textbf{Definition:} Analogical reasoning applies knowledge from one context to another by recognizing similar patterns or relationships.

\textbf{Applications in AI/ML:}
\begin{itemize}
    \item AI-powered tutoring systems
    \item Cross-domain learning
    \item Transfer learning in neural networks
\end{itemize}

\textbf{Example:} AI that learns human speech patterns in English and transfers that learning to generate speech in another language.

\subsubsection{Bayesian Reasoning (Probabilistic Prediction)}
\textbf{Definition:} Bayesian reasoning uses probability to predict outcomes by updating beliefs based on new evidence.

\textbf{Applications in AI/ML:}
\begin{itemize}
    \item Spam filtering systems
    \item AI language models
    \item Uncertainty quantification
\end{itemize}

\textbf{Example:} A Bayesian spam filter assigns probabilities to words appearing in spam emails and calculates the likelihood that a new email is spam.

\subsubsection{Causal Reasoning (Understanding Cause-and-Effect)}
\textbf{Definition:} Causal reasoning determines causal relationships rather than just correlations.

\textbf{Limitations in Current AI:}
\begin{itemize}
    \item LLMs struggle with true causality
    \item Most AI systems identify correlations rather than causes
\end{itemize}

\textbf{Example:} In healthcare, researchers identify that smoking causes lung cancer, rather than just observing that smokers have higher cancer rates.

\subsection{Reasoning Capabilities Across Intelligence Types}
The table below compares how different types of intelligence systems handle various reasoning tasks:

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Reasoning Type} & \textbf{Animals} & \textbf{Large Language Models} & \textbf{Traditional AI} \\
\hline
\textbf{Inductive} & Primary (survival-focused) & Primary (pattern-based) & Limited \\
\hline
\textbf{Deductive} & Absent (complex forms) & Simulated (pattern matching) & Primary (rule-based) \\
\hline
\textbf{Abductive} & Limited (simple forms) & Effective (learned patterns) & Limited \\
\hline
\textbf{Causal} & Basic & Limited & Rule-dependent \\
\hline
\textbf{Adaptability} & High (within domain) & High (pattern recognition) & Low (manual updates) \\
\hline
\end{tabular}
\caption{Reasoning capabilities across different types of intelligence systems.}
\end{table}

While inductive reasoning is powerful, it has inherent limitations that both animals and AI systems must navigate.

\subsection{Limitations of Inductive Reasoning}

\subsubsection{Pigeon Superstition Experiment (B.F. Skinner)}
This experiment demonstrated how animals can form false associations through inductive reasoning. Pigeons were given food at random intervals, leading them to develop "superstitious" behaviors---repeating whatever action they happened to be performing when food appeared, even though these actions had no causal relationship to receiving food.

\subsubsection{Garcia \& Koelling Experiment (1966)}
This landmark experiment studied \textbf{selective associative learning} in rats and demonstrated that \textbf{not all stimuli are equally associated} with consequences.

\textbf{Experimental Design:}
Researchers used a compound stimulus approach:
\begin{itemize}
    \item \textbf{Taste component}: Saccharin-flavored water
    \item \textbf{Audiovisual component}: Lights and sounds during drinking
\end{itemize}

Rats were then exposed to different aversive consequences:
\begin{itemize}
    \item \textbf{Group 1}: Illness (nausea from mild radiation or toxin)
    \item \textbf{Group 2}: Physical discomfort (mild electric shocks)
\end{itemize}

\textbf{Results:}

\textbf{Illness-Induced Group:}
\begin{itemize}
    \item Developed strong aversion to taste cues (saccharin water)
    \item Showed minimal aversion to audiovisual cues
\end{itemize}

\textbf{Shock-Induced Group:}
\begin{itemize}
    \item Developed strong aversion to audiovisual cues (lights and sounds)
    \item Showed no aversion to taste cues
\end{itemize}

\textbf{Key Finding:} Rats selectively associated specific stimuli with appropriate consequences---taste with illness, external cues with physical danger.

\textbf{Scientific Impact:}
This experiment revolutionized learning theory by:
\begin{itemize}
    \item \textbf{Challenging equipotentiality}: Not all stimulus-response associations are equally learnable
    \item \textbf{Demonstrating biological constraints}: Evolution shapes what animals can easily learn
    \item \textbf{Revealing adaptive biases}: Learning mechanisms evolved to enhance survival
    \begin{itemize}
        \item Taste naturally links to internal consequences (poisoning)
        \item External cues (sounds, lights) link to external threats (predators)
    \end{itemize}
\end{itemize}

\textbf{Implications for Machine Learning:}
\begin{itemize}
    \item \textbf{Learning requires inductive bias}: Not all associations are equally learnable
    \item \textbf{Feature relevance varies}: Some inputs are more informative than others
    \item \textbf{Domain knowledge matters}: Evolutionary or expert-designed constraints improve learning
    \item \textbf{No universal learner exists}: All learning algorithms must make assumptions (No-Free-Lunch theorem)
\end{itemize}

These biological insights directly inform machine learning design, where inductive bias plays a crucial role in model performance.

\section{Inductive Bias in Machine Learning}
\subsection{What is Inductive Bias?}
\textbf{Definition:} Inductive bias refers to the set of assumptions that a learning algorithm makes to generalize from limited training data to unseen data.

\textbf{Why is it Critical?}
Inductive bias is essential because:
\begin{itemize}
    \item Machine learning models have limited training data
    \item Models must generalize from past observations to unseen cases
    \item Without appropriate bias, models may overfit (memorizing training data without learning generalizable patterns)
    \item All successful learning algorithms require appropriate assumptions about their domain
\end{itemize}

\subsection{Types of Inductive Biases}

\subsubsection{Preference for Simpler Models (Occam's Razor)}
\begin{itemize}
    \item \textbf{Assumption}: Simpler explanations are preferred over complex ones
    \item \textbf{Example}: Decision trees with fewer splits are preferred because they generalize better
    \item \textbf{In Deep Learning}: Regularization techniques (L1, L2) penalize complex models
\end{itemize}

\subsubsection{Smoothness Assumption}
\begin{itemize}
    \item \textbf{Assumption}: Data points that are close together should have similar outputs
    \item \textbf{Example}: In image classification, two similar images should belong to the same class
    \item \textbf{In ML}: K-Nearest Neighbors (KNN) assumes nearby data points have the same label
\end{itemize}

\subsubsection{Similar Features Should Have Similar Effects}
\begin{itemize}
    \item \textbf{Assumption}: If two features are related, their effects should be similar
    \item \textbf{Example}: In linear regression, correlated features often have similar coefficients
\end{itemize}

\subsubsection{Prior Knowledge About the Task (Domain-Specific Bias)}
\begin{itemize}
    \item \textbf{Assumption}: Certain relationships are more likely in specific tasks
    \item \textbf{Example}: In NLP, word order matters
    \item \textbf{In ML}: Transformers use positional embeddings to capture sentence structure
\end{itemize}

\subsubsection{Invariance Bias (Translation, Rotation, Scale Invariance)}
\begin{itemize}
    \item \textbf{Assumption}: Some transformations should not change predictions
    \item \textbf{Example}: Rotating an image of a cat should still classify it as a cat
    \item \textbf{In ML}: CNNs use convolutional filters to enforce translation invariance
\end{itemize}

\subsubsection{Sparsity Assumption}
\begin{itemize}
    \item \textbf{Assumption}: Only a few features are truly important
    \item \textbf{Example}: In text classification, most words are irrelevant
    \item \textbf{In ML}: L1 regularization forces models to select important features
\end{itemize}

These general principles manifest differently across various neural network architectures, each designed with specific inductive biases for particular tasks.

\subsection{Inductive Bias in Specific Architectures}
\subsubsection{Convolutional Neural Networks (CNNs)}
CNNs are designed for image processing and rely on key inductive biases:

\textbf{1. Locality Bias (Local Connectivity)}
\begin{itemize}
    \item \textbf{Assumption}: Nearby pixels are more relevant than distant pixels
    \item \textbf{Example}: In facial recognition, CNN detects eyes, nose, mouth before recognizing entire face
\end{itemize}

\textbf{2. Translation Invariance}
\begin{itemize}
    \item \textbf{Assumption}: An object should be recognized regardless of position
    \item \textbf{How it works}: CNNs use shared convolutional filters
    \item \textbf{Example}: Handwritten digit "3" recognized anywhere in the image
\end{itemize}

\textbf{3. Hierarchical Feature Learning}
\begin{itemize}
    \item \textbf{Assumption}: Complex patterns learned by stacking abstraction layers
    \item \textbf{Example}: Lower layers detect edges $\rightarrow$ middle layers detect shapes $\rightarrow$ deeper layers detect objects
\end{itemize}

\subsubsection{Recurrent Neural Networks (RNNs \& LSTMs)}
RNNs are designed for sequential data and rely on:

\textbf{1. Temporal Dependency Bias}
\begin{itemize}
    \item \textbf{Assumption}: Recent information is more important than distant past
    \item \textbf{Example}: In "The cat sat on the mat", nearby words are more related
\end{itemize}

\textbf{2. Order Sensitivity Bias}
\begin{itemize}
    \item \textbf{Assumption}: The order of input elements matters
    \item \textbf{Example}: "Dog bites man" $\neq$ "Man bites dog"
\end{itemize}

\subsubsection{Transformers (BERT, GPT)}

\textbf{1. Attention-Based Bias (Self-Attention)}
\begin{itemize}
    \item \textbf{Assumption}: Important words can be anywhere in a sentence
    \item \textbf{Example}: In "The dog chased the ball...which was blue", "which" refers to "ball"
\end{itemize}

\textbf{2. Context-Aware Learning Bias}
\begin{itemize}
    \item \textbf{Assumption}: Word meaning depends on context
    \item \textbf{Example}: "Bank" can mean financial institution or riverbank
\end{itemize}

\textbf{3. Positional Encoding Bias}
\begin{itemize}
    \item \textbf{Assumption}: Order matters even without sequential processing
    \item \textbf{Example}: "She ate an apple" $\neq$ "An apple ate she"
\end{itemize}

\section{Mathematical Foundations of Learning}
\subsection{Learning as Optimization}
Machine learning can be viewed as an optimization problem where we seek to find the best parameters \(\theta\) that minimize a loss function \(L(\theta)\):
\[\theta^* = \arg\min_\theta L(\theta)\]

\subsubsection{Components of a Learning System}
\begin{enumerate}
    \item \textbf{Hypothesis Space} \(\mathcal{H}\): The set of all possible functions the model can represent
    \item \textbf{Loss Function} \(L(\theta)\): Measures how well the model performs on the training data
    \item \textbf{Optimization Algorithm}: Method to find \(\theta^*\) (e.g., gradient descent)
    \item \textbf{Regularization}: Techniques to prevent overfitting and improve generalization
\end{enumerate}

\subsubsection{The Bias-Variance Tradeoff}
The expected prediction error can be decomposed as:
\[\text{Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}\]
where:
\begin{itemize}
    \item \textbf{Bias}: Error due to simplifying assumptions in the model
    \item \textbf{Variance}: Error due to sensitivity to small fluctuations in training data
    \item \textbf{Irreducible Error}: Inherent noise in the data
\end{itemize}

\subsection{PAC Learning Framework}
\textbf{Probably Approximately Correct (PAC)} learning provides theoretical foundations for when learning is possible.

A concept class \(\mathcal{C}\) is PAC-learnable if there exists an algorithm that, for any distribution \(\mathcal{D}\) and any \(\epsilon, \delta > 0\), can find a hypothesis \(h\) such that:
\[\Pr[\text{error}(h) \leq \epsilon] \geq 1 - \delta\]
using polynomially many samples and computational steps.

\section{Symbolic AI vs Machine Learning}
\subsection{What is Symbolic AI?}
Also known as \textbf{Good Old-Fashioned AI (GOFAI)}, it represents knowledge using symbols, rules, and logic. It uses explicitly programmed rules for reasoning.

\subsection{Symbolic AI vs Machine Learning Comparison}
\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Feature} & \textbf{Symbolic AI} & \textbf{Machine Learning} \\
\hline
Knowledge Source & Rules \& logic & Data \& patterns \\
Interpretability & Highly explainable & Often a black box \\
Adaptability & Rigid (manual updates) & Can generalize from data \\
Data Requirements & Minimal & Requires large datasets \\
Best Use Cases & Theorem proving & NLP, computer vision \\
\hline
\end{tabular}
\caption{Comparison of Symbolic AI and Machine Learning.}
\end{table}
